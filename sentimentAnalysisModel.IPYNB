{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sentiment Analysis Model - TI2**\n",
    "The goal of this project is to build a sentiment analysis model using supervised learning with vanilla Recurrent Neural Networks and LSTM.\n",
    "\n",
    "\n",
    "## **Preprocess:**\n",
    "\n",
    "Neceesary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Cristian\n",
      "[nltk_data]     Perafan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Cristian\n",
      "[nltk_data]     Perafan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "# Necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English stopwords: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data into a pandas dataframe, where each row is a sentence and each column is a label (0 for negative, 1 for positive) and the text itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df = pd.read_csv('./sentiment labelled sentences/amazon_cells_labelled.txt', sep='\\t',names=['sentence', 'tag'])\n",
    "\n",
    "imdb_df = pd.read_csv('./sentiment labelled sentences/imdb_labelled.txt', sep='\\t',names=['sentence', 'tag'])\n",
    "\n",
    "yelp_df = pd.read_csv('./sentiment labelled sentences/yelp_labelled.txt', sep='\\t',names=['sentence', 'tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize and delete the stop words from text data  using NLTK:\n",
    "\n",
    "- *word.isalnum()* ensures that only words containing alphabetic or numeric characters are included and excludes punctuation marks or other special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize and delete stop words from the Amazon sentences\n",
    "\n",
    "amazon_df['tokens'] = amazon_df['sentence'].apply(lambda x: [word.lower() for word in word_tokenize(x) if word.isalnum() and word.lower() not in stop_words])\n",
    "\n",
    "\n",
    "# Tokenize and delete stop words from the IMDB sentences\n",
    "imdb_df['tokens'] = imdb_df['sentence'].apply(lambda x: [word.lower() for word in word_tokenize(x) if word.isalnum() and word.lower() not in stop_words])\n",
    "\n",
    "# Tokenize and delete stop words from the Yelp sentences\n",
    "yelp_df['tokens'] = yelp_df['sentence'].apply(lambda x: [word.lower() for word in word_tokenize(x) if word.isalnum() and word.lower() not in stop_words])\n",
    "\n",
    "\n",
    "combined_sentiments_df = pd.concat([amazon_df, imdb_df, yelp_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(combined_sentiments_df['tokens'], combined_sentiments_df['tag'], test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DummyClassifier**\n",
    "\n",
    "Neceesary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the DummyClassifier model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dummy = DummyClassifier(random_state=43,strategy='prior')\n",
    "clf_dummy.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = clf_dummy.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model performance**\n",
    "\n",
    "*Accuracy*: is the fraction of predictions our model got right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4727272727272727"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Presicion*: is the fraction of positive predictions that are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cristian Perafan\\VC workspace\\sentiment-analysisModel-ti2\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.47272727])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(Y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recaal*: is the fraction of the truly positive instances that the classifier recognizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(Y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*f1-score*: is the harmonic mean of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.64197531])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RNN Model**\t\n",
    "\n",
    "Neceesary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Cristian Perafan\\VC workspace\\sentiment-analysisModel-ti2\\venv\\Lib\\site-packages\\keras\\losses.py:2664: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the RNN model and adding layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Cristian Perafan\\VC workspace\\sentiment-analysisModel-ti2\\venv\\Lib\\site-packages\\keras\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 64)                5952      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64)               256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,858\n",
      "Trainable params: 6,730\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.SimpleRNN(64, input_shape=(None, 28)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(10))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Cristian Perafan\\VC workspace\\sentiment-analysisModel-ti2\\venv\\Lib\\site-packages\\keras\\optimizers\\__init__.py:300: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactoring the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model:\n",
    "\n",
    "- Creating a Tokenizer object. Tokenizer is a Keras class used to convert text into sequences of tokens (words). This object will contain information about the vocabulary of the data set and will allow text to be transformed into sequences of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000,lower=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method examines the text and builds the tokenizer's internal vocabulary, assigning a unique index to each word in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning each word in the vocabulary will be assigned a unique integer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequence_of_integers = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequence_of_integers = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train_sequence_of_integers_padded = pad_sequences(X_train_sequence_of_integers, padding='post', maxlen=maxlen)\n",
    "\n",
    "\n",
    "X_test_sequence_of_integers_padded = pad_sequences(X_test_sequence_of_integers, padding='post', maxlen=maxlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, SimpleRNN,Activation\n",
    "from keras import optimizers\n",
    "from keras.utils import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X_train_np = np.array(X_train_sequence_of_integers_padded).reshape(X_train_sequence_of_integers_padded.shape[0],X_train_sequence_of_integers_padded.shape[1],1)\n",
    "\n",
    "\n",
    "X_test_np = np.array(X_test_sequence_of_integers_padded).reshape(X_test_sequence_of_integers_padded.shape[0],X_test_sequence_of_integers_padded.shape[1],1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vainilla RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "def vainilla_rnn():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(50, input_shape=(maxlen,1),return_sequences=False))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    adam= optimizers.Adam(lr=0.001)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cristian Perafan\\AppData\\Local\\Temp\\ipykernel_6900\\2424741882.py:5: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=vainilla_rnn, epochs=10, batch_size=32, verbose=1)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "model = KerasClassifier(build_fn=vainilla_rnn, epochs=10, batch_size=32, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_2 (SimpleRNN)    (None, 50)                2600      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 102       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,702\n",
      "Trainable params: 2,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\Cristian Perafan\\VC workspace\\sentiment-analysisModel-ti2\\venv\\Lib\\site-packages\\keras\\utils\\tf_utils.py:490: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Cristian Perafan\\VC workspace\\sentiment-analysisModel-ti2\\venv\\Lib\\site-packages\\keras\\engine\\base_layer_utils.py:380: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "61/61 [==============================] - 2s 11ms/step - loss: 0.6991 - accuracy: 0.4971\n",
      "Epoch 2/10\n",
      "61/61 [==============================] - 1s 11ms/step - loss: 0.6953 - accuracy: 0.4997\n",
      "Epoch 3/10\n",
      "61/61 [==============================] - 1s 11ms/step - loss: 0.6936 - accuracy: 0.5086\n",
      "Epoch 4/10\n",
      "61/61 [==============================] - 1s 11ms/step - loss: 0.6956 - accuracy: 0.5039\n",
      "Epoch 5/10\n",
      "61/61 [==============================] - 1s 10ms/step - loss: 0.6944 - accuracy: 0.5023\n",
      "Epoch 6/10\n",
      "61/61 [==============================] - 1s 10ms/step - loss: 0.6937 - accuracy: 0.5101\n",
      "Epoch 7/10\n",
      "61/61 [==============================] - 1s 10ms/step - loss: 0.6943 - accuracy: 0.5070\n",
      "Epoch 8/10\n",
      "61/61 [==============================] - 1s 10ms/step - loss: 0.6993 - accuracy: 0.4857\n",
      "Epoch 9/10\n",
      "61/61 [==============================] - 1s 11ms/step - loss: 0.6987 - accuracy: 0.5086\n",
      "Epoch 10/10\n",
      "61/61 [==============================] - 1s 10ms/step - loss: 0.6984 - accuracy: 0.5044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bc590db210>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(X_train_np, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 6ms/step\n",
      "0.5078787878787879\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_np)\n",
    "\n",
    "print(accuracy_score(Y_test, y_pred))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
